# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html


# useful for handling different item types with a single interface
from itemadapter import ItemAdapter

import pymysql
import json

#æ•°æ®åº“å¯†ç 
mysql_password = ''
#æ•°æ®åº“ç”¨æˆ·å
mysql_username = ''
#æ•°æ®åº“å
database_name = ''
class TiebaPipeline:
    flag = 0
    def open_spider(self,spider):
        self.mysql = pymysql.connect('localhost',mysql_username,mysql_password,database_name)
        self.cursor = self.mysql.cursor()

    def process_item(self, item, spider):
        item,judge = self.deal_with_info(item)
        if judge:
            sqlcmd = f'''insert into tbinfo values(0,'{item['title']}','{item['auther']}','{item['detail_url']}',{item['article_id']},{item['is_top']},'{item['content']}','{item['img_list']}','{item['upload_time']}')'''
            try:
                self.cursor.execute(sqlcmd)
                self.mysql.commit()
            except:
                self.mysql.rollback()
        return item

    def deal_with_info(self,info):
        #ğŸ’å­—ç¬¦æ— æ³•å­˜å…¥mysql,æ•…éœ€è¦å»é™¤
        for key in info:
            try:
                info[key] = info[key].replace('ğŸ’','')
            except:
                continue
        info['content'] = info['content'].strip()
        info['img_list'] = json.dumps(info['img_list'])
        # from datetime import datetime
        # info['upload_time'] = datetime.strptime(info['upload_time'],'%Y-%m-%d %H:%M')

        #å»é™¤é‡å¤çš„ç½®é¡¶å¸–
        if self.flag == 1:
            if self.info['title'] == info['title']:
                return (info,False)
        if info['is_top'] == 1 and self.flag == 0:
            self.info = info
            self.flag = 1
        return (info,True)

    def close_spider(self,spider):
        self.cursor.close()
        self.mysql.close()


